# Knowledge Base MVP

Um MVP completo de Knowledge Base + Web Crawler + FAQs para negÃ³cios de consignado (INSS portabilidade) e FGTS, com integraÃ§Ã£o ao Dify para alimentar agentes de IA.

## ğŸš€ Funcionalidades

- **Cadastro de Fontes**: Gerencie URLs para crawl
- **Web Crawler Dual**: Modo direto (Firecrawl) e via n8n webhook
- **CRUD de FAQs**: Interface completa para gerenciar perguntas frequentes
- **Status de Jobs**: Acompanhe o progresso dos crawls em tempo real
- **IntegraÃ§Ã£o Dify**: Duas formas de integraÃ§Ã£o (Datasets nativos + External Knowledge API)
- **Supabase + pgvector**: Banco preparado para RAG com embeddings

## ğŸ› ï¸ Stack TecnolÃ³gica

- **Frontend**: Next.js 14 (App Router), TypeScript, Tailwind CSS
- **Backend**: Next.js API Routes, Supabase JS Client
- **Banco**: Supabase (PostgreSQL + pgvector)
- **Crawler**: Firecrawl (SDK) + n8n (webhook)
- **IA**: Dify (Datasets + External Knowledge API)

## ğŸ“‹ PrÃ©-requisitos

- Node.js 18+
- Conta Supabase
- API Key do Firecrawl
- n8n instance (opcional)
- Dify instance

## âš™ï¸ ConfiguraÃ§Ã£o

### 1. Clone e instale dependÃªncias

\`\`\`bash
git clone <repository-url>
cd knowledge-base
npm install
\`\`\`

### 2. Configure variÃ¡veis de ambiente

Copie \`env.example\` para \`.env.local\`:

\`\`\`bash
cp env.example .env.local
\`\`\`

Configure as variÃ¡veis:

\`\`\`env
# Supabase Configuration
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_ROLE=your_service_role_key_here

# Firecrawl API
FIRECRAWL_API_KEY=your_firecrawl_api_key_here

# n8n Webhook (opcional)
N8N_CRAWL_WEBHOOK=https://your-n8n-instance.com/webhook/crawl

# Dify Configuration
DIFY_API_BASE=https://your-dify-instance.com/v1
DIFY_API_KEY=your_dify_api_key_here
EXTERNAL_KB_API_KEY=your_external_kb_api_key_here
\`\`\`

### 3. Configure o banco Supabase

As migraÃ§Ãµes jÃ¡ foram aplicadas automaticamente:

- âœ… ExtensÃ£o pgvector habilitada
- âœ… Tabelas criadas com RLS ativo
- âœ… PolÃ­ticas de seguranÃ§a configuradas

### 4. Execute o projeto

\`\`\`bash
npm run dev
\`\`\`

Acesse: http://localhost:3000

## ğŸ—„ï¸ Estrutura do Banco

### Tabelas Principais

- **kb_sources**: Fontes de conhecimento (URLs)
- **crawl_jobs**: Jobs de crawl com status
- **documents**: Documentos extraÃ­dos (markdown)
- **chunks**: PedaÃ§os dos documentos para RAG
- **embeddings**: Vetores para busca semÃ¢ntica (futuro)
- **faqs**: Perguntas frequentes

### RLS (Row Level Security)

- **faqs**: Leitura pÃºblica, escrita via service role
- **Demais tabelas**: Acesso apenas via service role (backend)

## ğŸ”Œ Endpoints da API

### Knowledge Base

- \`POST /api/kb/source\` - Criar fonte
- \`GET /api/kb/source\` - Listar fontes
- \`POST /api/kb/crawl\` - Iniciar crawl
- \`GET /api/kb/jobs/:id\` - Status do job
- \`GET /api/kb/faqs\` - Listar FAQs
- \`POST /api/kb/faqs\` - Criar FAQ
- \`PATCH /api/kb/faqs/:id\` - Atualizar FAQ
- \`DELETE /api/kb/faqs/:id\` - Deletar FAQ

### Dify Integration

- \`GET /api/dify/datasets\` - Listar datasets
- \`POST /api/dify/datasets\` - Criar dataset
- \`POST /api/dify/retrieval\` - External Knowledge API

## ğŸ•·ï¸ Web Crawler

### Modo Direct (Firecrawl)

- Chamada direta Ã  API do Firecrawl
- Retorna markdown estruturado
- Processamento em background
- Chunking automÃ¡tico

### Modo n8n

- Webhook para workflow n8n
- Resposta assÃ­ncrona
- Ideal para workloads longos
- ConfigurÃ¡vel via N8N_CRAWL_WEBHOOK

## ğŸ¤– IntegraÃ§Ã£o Dify

### 1. Datasets Nativos

Use os endpoints \`/api/dify/datasets\` para:
- Listar datasets existentes
- Criar novos datasets
- Upload de documentos

### 2. External Knowledge API

Endpoint \`/api/dify/retrieval\`:
- Recebe queries do Dify
- Retorna chunks relevantes do Supabase
- AutenticaÃ§Ã£o via Bearer token
- Formato compatÃ­vel com Dify

**Como configurar no Dify:**
1. Acesse Knowledge Management
2. Adicione External Knowledge API
3. URL: \`https://your-domain.com/api/dify/retrieval\`
4. API Key: \`EXTERNAL_KB_API_KEY\`

## ğŸ“± Interface Administrativa

### Card 1: Knowledge Sources
- Adicionar URLs
- Configurar escopo (single/path/domain)
- Definir profundidade do crawl
- Listar fontes existentes

### Card 2: Web Crawler
- Selecionar modo (direct/n8n)
- Escolher fonte para crawl
- Acompanhar status dos jobs
- HistÃ³rico de execuÃ§Ãµes

### Card 3: FAQs
- CRUD completo de FAQs
- Interface intuitiva
- EdiÃ§Ã£o inline
- ValidaÃ§Ã£o de campos

## ğŸ”§ Desenvolvimento

### Scripts DisponÃ­veis

\`\`\`bash
npm run dev      # Desenvolvimento
npm run build    # Build de produÃ§Ã£o
npm run start    # Servidor de produÃ§Ã£o
npm run lint     # Linting
\`\`\`

### Estrutura de Pastas

\`\`\`
src/
â”œâ”€â”€ app/                 # Next.js App Router
â”‚   â”œâ”€â”€ api/            # API Routes
â”‚   â”œâ”€â”€ globals.css     # Estilos globais
â”‚   â”œâ”€â”€ layout.tsx      # Layout principal
â”‚   â””â”€â”€ page.tsx        # PÃ¡gina principal
â”œâ”€â”€ components/         # Componentes React
â”‚   â””â”€â”€ ui/            # Componentes base
â””â”€â”€ lib/               # UtilitÃ¡rios e configuraÃ§Ãµes
    â”œâ”€â”€ types.ts       # Tipos TypeScript
    â”œâ”€â”€ supabase.ts    # Cliente Supabase
    â”œâ”€â”€ firecrawl.ts   # Cliente Firecrawl
    â”œâ”€â”€ dify.ts        # Cliente Dify
    â””â”€â”€ chunking.ts    # LÃ³gica de chunking
\`\`\`

## ğŸš€ Deploy

### Vercel (Recomendado)

1. Conecte seu repositÃ³rio ao Vercel
2. Configure as variÃ¡veis de ambiente
3. Deploy automÃ¡tico

### Outras Plataformas

O projeto Ã© compatÃ­vel com qualquer plataforma que suporte Next.js:
- Netlify
- Railway
- DigitalOcean App Platform
- AWS Amplify

## ğŸ” SeguranÃ§a

- RLS ativo em todas as tabelas
- Service role apenas no backend
- ValidaÃ§Ã£o de entrada em todos os endpoints
- AutenticaÃ§Ã£o via API keys para integraÃ§Ãµes externas

## ğŸ“Š Monitoramento

- Logs estruturados em console
- Status de jobs em tempo real
- Tratamento de erros robusto
- Mensagens de feedback na UI

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature
3. Commit suas mudanÃ§as
4. Push para a branch
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT.

## ğŸ†˜ Suporte

Para dÃºvidas ou problemas:
1. Verifique os logs do console
2. Confirme as variÃ¡veis de ambiente
3. Teste os endpoints individualmente
4. Abra uma issue no repositÃ³rio

---

**Desenvolvido para otimizar o pipeline de conhecimento e alimentar agentes de IA com dados estruturados e atualizados.**

